{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "1bfc5541-498c-43fb-eaa5-6953be23d2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24559 sha256=d07c210cc7d46cca309979796977d4129c9fc84423255e04805302444261c7e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "29485244-0d13-4a30-eea2-199f3f2e7b2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 675Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 567kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 282Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:59, 8.38Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 492Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 715kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 830kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "5250760c-6785-4512-c817-b023d8f98559"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "b4451caa-7609-4f60-a73a-be427eb8c40a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "To be honest, I'm not sure that I would have considered the chance of it happening if the planet had been visited by Ancient Aliens. I'm actually quite certain that it's not. I mean, if it was the Earth, I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "ffa95e88-3ee1-4692-c76c-a14729c264b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-31 12:01:34--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.38.54, 52.217.110.254, 54.231.193.64, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.38.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K   534KB/s    in 1.1s    \n",
            "\n",
            "2023-05-31 12:01:36 (534 KB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "4b6dc3e6-5254-4daf-b3df-ec1905a497c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 143MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "1be84568-e4e2-4364-b1cf-1fff6167538b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 6.37] loss=3.41 avg=3.41\n",
            "[2 | 8.67] loss=3.31 avg=3.36\n",
            "[3 | 10.99] loss=3.43 avg=3.38\n",
            "[4 | 13.32] loss=3.40 avg=3.39\n",
            "[5 | 15.68] loss=3.46 avg=3.40\n",
            "[6 | 18.06] loss=3.25 avg=3.38\n",
            "[7 | 20.46] loss=3.19 avg=3.35\n",
            "[8 | 22.88] loss=3.16 avg=3.32\n",
            "[9 | 25.30] loss=3.42 avg=3.33\n",
            "[10 | 27.73] loss=3.16 avg=3.32\n",
            "[11 | 30.12] loss=3.31 avg=3.32\n",
            "[12 | 32.49] loss=3.26 avg=3.31\n",
            "[13 | 34.84] loss=3.20 avg=3.30\n",
            "[14 | 37.18] loss=3.12 avg=3.29\n",
            "[15 | 39.50] loss=3.11 avg=3.28\n",
            "[16 | 41.81] loss=3.24 avg=3.27\n",
            "[17 | 44.12] loss=3.05 avg=3.26\n",
            "[18 | 46.41] loss=3.11 avg=3.25\n",
            "[19 | 48.69] loss=2.95 avg=3.23\n",
            "[20 | 50.99] loss=3.15 avg=3.23\n",
            "[21 | 53.26] loss=3.14 avg=3.22\n",
            "[22 | 55.53] loss=3.14 avg=3.22\n",
            "[23 | 57.80] loss=3.09 avg=3.21\n",
            "[24 | 60.06] loss=2.94 avg=3.20\n",
            "[25 | 62.32] loss=3.13 avg=3.20\n",
            "[26 | 64.56] loss=3.08 avg=3.19\n",
            "[27 | 66.81] loss=3.12 avg=3.19\n",
            "[28 | 69.07] loss=3.12 avg=3.19\n",
            "[29 | 71.33] loss=3.15 avg=3.18\n",
            "[30 | 73.57] loss=3.10 avg=3.18\n",
            "[31 | 75.82] loss=3.12 avg=3.18\n",
            "[32 | 78.09] loss=3.23 avg=3.18\n",
            "[33 | 80.35] loss=3.09 avg=3.18\n",
            "[34 | 82.62] loss=3.17 avg=3.18\n",
            "[35 | 84.90] loss=3.04 avg=3.17\n",
            "[36 | 87.18] loss=3.00 avg=3.17\n",
            "[37 | 89.47] loss=3.18 avg=3.17\n",
            "[38 | 91.76] loss=3.01 avg=3.16\n",
            "[39 | 94.06] loss=3.11 avg=3.16\n",
            "[40 | 96.37] loss=2.96 avg=3.15\n",
            "[41 | 98.67] loss=2.95 avg=3.15\n",
            "[42 | 100.98] loss=3.00 avg=3.14\n",
            "[43 | 103.29] loss=2.97 avg=3.14\n",
            "[44 | 105.60] loss=3.01 avg=3.14\n",
            "[45 | 107.92] loss=2.99 avg=3.13\n",
            "[46 | 110.24] loss=3.06 avg=3.13\n",
            "[47 | 112.54] loss=2.93 avg=3.12\n",
            "[48 | 114.85] loss=2.91 avg=3.12\n",
            "[49 | 117.17] loss=3.01 avg=3.12\n",
            "[50 | 119.47] loss=2.95 avg=3.11\n",
            "[51 | 121.78] loss=3.06 avg=3.11\n",
            "[52 | 124.08] loss=3.02 avg=3.11\n",
            "[53 | 126.38] loss=3.00 avg=3.11\n",
            "[54 | 128.67] loss=2.89 avg=3.10\n",
            "[55 | 130.97] loss=2.97 avg=3.10\n",
            "[56 | 133.25] loss=2.84 avg=3.09\n",
            "[57 | 135.54] loss=3.01 avg=3.09\n",
            "[58 | 137.83] loss=2.91 avg=3.09\n",
            "[59 | 140.11] loss=2.84 avg=3.08\n",
            "[60 | 142.41] loss=2.95 avg=3.08\n",
            "[61 | 144.69] loss=2.91 avg=3.07\n",
            "[62 | 146.97] loss=2.80 avg=3.07\n",
            "[63 | 149.25] loss=2.90 avg=3.06\n",
            "[64 | 151.53] loss=2.94 avg=3.06\n",
            "[65 | 153.81] loss=3.02 avg=3.06\n",
            "[66 | 156.09] loss=3.00 avg=3.06\n",
            "[67 | 158.37] loss=2.84 avg=3.05\n",
            "[68 | 160.64] loss=2.92 avg=3.05\n",
            "[69 | 162.92] loss=2.92 avg=3.05\n",
            "[70 | 165.20] loss=2.87 avg=3.05\n",
            "[71 | 167.50] loss=2.74 avg=3.04\n",
            "[72 | 169.78] loss=2.82 avg=3.04\n",
            "[73 | 172.06] loss=2.87 avg=3.03\n",
            "[74 | 174.35] loss=2.93 avg=3.03\n",
            "[75 | 176.63] loss=2.84 avg=3.03\n",
            "[76 | 178.92] loss=2.84 avg=3.02\n",
            "[77 | 181.21] loss=2.92 avg=3.02\n",
            "[78 | 183.49] loss=2.92 avg=3.02\n",
            "[79 | 185.78] loss=2.89 avg=3.02\n",
            "[80 | 188.05] loss=2.87 avg=3.01\n",
            "[81 | 190.34] loss=2.80 avg=3.01\n",
            "[82 | 192.62] loss=2.87 avg=3.01\n",
            "[83 | 194.91] loss=2.83 avg=3.00\n",
            "[84 | 197.20] loss=2.68 avg=3.00\n",
            "[85 | 199.47] loss=2.81 avg=3.00\n",
            "[86 | 201.75] loss=2.97 avg=3.00\n",
            "[87 | 204.04] loss=2.78 avg=2.99\n",
            "[88 | 206.33] loss=2.72 avg=2.99\n",
            "[89 | 208.61] loss=2.82 avg=2.98\n",
            "[90 | 210.90] loss=2.93 avg=2.98\n",
            "[91 | 213.18] loss=2.95 avg=2.98\n",
            "[92 | 215.47] loss=2.82 avg=2.98\n",
            "[93 | 217.76] loss=2.77 avg=2.98\n",
            "[94 | 220.05] loss=2.79 avg=2.97\n",
            "[95 | 222.33] loss=2.97 avg=2.97\n",
            "[96 | 224.61] loss=2.91 avg=2.97\n",
            "[97 | 226.90] loss=2.77 avg=2.97\n",
            "[98 | 229.20] loss=2.66 avg=2.96\n",
            "[99 | 231.48] loss=2.85 avg=2.96\n",
            "[100 | 233.77] loss=2.81 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " be called the \"Great Dragon.\" He had been called it since the gods permitted it. \n",
            "He had the headstrong eyes and a great fear-of-death, and he was afraid to say so . . . unless . . . until . . . . before the \n",
            "first dragon, but never, ever. All was not lost. The king was holding his horses as the sun went down and \n",
            "frozen it in place. He knew that if he put the horses in the dragon's belly, the dragon would come. If the \n",
            "sun had gone out with nothing to do except sleep on, they were going to be screaming and wailing, \n",
            "and Robert's brother would hear them, but it was none like that, and soon he could feel it in him too. He \n",
            "felt it. The wind whipped up, his fingers digging out the dirt clinging to her arm. He felt it. \n",
            "It was that breath that frightened him, and it was that breath that made him cry. \n",
            "And then, as it went out, the dragon flew like a thundercloud. Robert looked down. \n",
            "\"I know . . . \" he whispered back to himself sadly, \"that's when you went to my mother and told her about \n",
            "my father and her . . . then you had my own thoughts, the other gods told me to. Now you talk of the \n",
            "great dragon, so much I miss that, but . . . . . when you were a boy, you made me think of you, I \n",
            "thought of you . . . \n",
            "\"He is \n",
            "that dragon,\" he said suddenly, \"he lives with me, it \n",
            "doesn't matter . . . it doesn't matter, it stays with me.\" His voice was soft and \n",
            "silent, and he smelled the roses. He felt them and prayed that his mother would come home. \n",
            "\"As you wish,\" Robert told him, a thousand times. He felt the cold in his eyes, and the way the \n",
            "wind blew through his nose. He felt the fear in the shadows, and suddenly he had a word with the dragon that went out like \n",
            "that. He would have to swear an oath to be kept from the dragon at all, but the word would take him \n",
            "too fast. He had to take the oath. He had chosen his words. So he swore. \n",
            "It was a time for many promises, and for the betterment of the realm, if Robert wished to be so . . . yet Robert was still \n",
            "still a child. And when he had been little, he had not given Robert the hint that he was a prince, even if he . . . \n",
            "\"I will make a vow of silence against the dragon,\" Robert said sharply. \"He has . . . . I have known you all my life, and \n",
            "all you do is speak and lie, and I will speak and do, you know that, for the good of humanity.\" \n",
            "\"It seems to me my father's a terrible man,\" Robert said. \"The more I look at him, the more I think that he may \n",
            "have no fear of a dragon.\" \n",
            "\"I know it is him,\" Robert said as he reached for his sword. \"Yet . . . his honor is \n",
            "unbefitting . . . yet I will bear it for him. I promised him he would never harm me, and I will give him \n",
            "his due.\" \n",
            "Robert knew that if he went to his mother, he would find a man who knew the difference between fear and \n",
            "fear. He had not spoken of her since he and Tyrion were married. Cersei's daughter had grown \n",
            "older and wiser. Her husband was a knight before she married him. No doubt she had learned from them \n",
            "how to deal with the dragon when they taught him the arts of armor and spear-fishing, and the dragons were not \n",
            "too different in either. \"You must be careful,\" Robb said, \"you must learn to fight. You must learn.\" He held out his sword and \n",
            "slapped Robert on the shoulder. \"You will not make him kill me if I don't.\" \n",
            "Robert stepped back, and saw as his fingers met the stone of the sword. Tyrion was slashing at him with \n",
            "the knife, the sword he had taken from Robb's hand. He could feel blood oozing back from the wound. He could see blood as he \n",
            "kneeled over his wounds, as red as ice on an ice flagon, blood swirling in the wind of the battle \n",
            "and the chill of his blood. He tried his best to ignore the pain; but by the time they were done \n",
            "cutting, he was frightened, so frightened he could not move without fear of losing his breath. He wanted to \n",
            "see Robert, at least. \n",
            "Robert\n",
            "\n",
            "[101 | 248.12] loss=2.99 avg=2.96\n",
            "[102 | 250.40] loss=2.59 avg=2.95\n",
            "[103 | 252.70] loss=2.70 avg=2.95\n",
            "[104 | 254.98] loss=2.80 avg=2.95\n",
            "[105 | 257.26] loss=2.77 avg=2.95\n",
            "[106 | 259.55] loss=2.83 avg=2.94\n",
            "[107 | 261.83] loss=2.77 avg=2.94\n",
            "[108 | 264.12] loss=2.76 avg=2.94\n",
            "[109 | 266.40] loss=2.90 avg=2.94\n",
            "[110 | 268.68] loss=2.66 avg=2.93\n",
            "[111 | 270.96] loss=2.77 avg=2.93\n",
            "[112 | 273.24] loss=2.74 avg=2.93\n",
            "[113 | 275.53] loss=2.84 avg=2.93\n",
            "[114 | 277.81] loss=2.77 avg=2.92\n",
            "[115 | 280.09] loss=2.65 avg=2.92\n",
            "[116 | 282.37] loss=2.72 avg=2.92\n",
            "[117 | 284.65] loss=2.79 avg=2.92\n",
            "[118 | 286.92] loss=2.83 avg=2.91\n",
            "[119 | 289.20] loss=2.57 avg=2.91\n",
            "[120 | 291.48] loss=2.77 avg=2.91\n",
            "[121 | 293.76] loss=2.78 avg=2.91\n",
            "[122 | 296.05] loss=2.79 avg=2.90\n",
            "[123 | 298.33] loss=2.69 avg=2.90\n",
            "[124 | 300.62] loss=2.80 avg=2.90\n",
            "[125 | 302.91] loss=2.92 avg=2.90\n",
            "[126 | 305.20] loss=2.68 avg=2.90\n",
            "[127 | 307.49] loss=2.91 avg=2.90\n",
            "[128 | 309.78] loss=2.74 avg=2.90\n",
            "[129 | 312.07] loss=2.51 avg=2.89\n",
            "[130 | 314.37] loss=2.69 avg=2.89\n",
            "[131 | 316.65] loss=2.77 avg=2.89\n",
            "[132 | 318.95] loss=2.76 avg=2.88\n",
            "[133 | 321.23] loss=2.75 avg=2.88\n",
            "[134 | 323.53] loss=2.76 avg=2.88\n",
            "[135 | 325.83] loss=2.60 avg=2.88\n",
            "[136 | 328.12] loss=2.75 avg=2.87\n",
            "[137 | 330.41] loss=2.62 avg=2.87\n",
            "[138 | 332.71] loss=2.70 avg=2.87\n",
            "[139 | 335.00] loss=2.62 avg=2.87\n",
            "[140 | 337.30] loss=2.83 avg=2.87\n",
            "[141 | 339.60] loss=2.56 avg=2.86\n",
            "[142 | 341.90] loss=2.70 avg=2.86\n",
            "[143 | 344.19] loss=2.73 avg=2.86\n",
            "[144 | 346.48] loss=2.47 avg=2.85\n",
            "[145 | 348.78] loss=2.66 avg=2.85\n",
            "[146 | 351.08] loss=2.72 avg=2.85\n",
            "[147 | 353.37] loss=2.78 avg=2.85\n",
            "[148 | 355.67] loss=2.78 avg=2.85\n",
            "[149 | 357.97] loss=2.71 avg=2.84\n",
            "[150 | 360.26] loss=2.80 avg=2.84\n",
            "[151 | 362.56] loss=2.75 avg=2.84\n",
            "[152 | 364.86] loss=2.67 avg=2.84\n",
            "[153 | 367.16] loss=2.80 avg=2.84\n",
            "[154 | 369.45] loss=2.74 avg=2.84\n",
            "[155 | 371.74] loss=2.59 avg=2.84\n",
            "[156 | 374.04] loss=2.67 avg=2.83\n",
            "[157 | 376.33] loss=2.54 avg=2.83\n",
            "[158 | 378.62] loss=2.61 avg=2.83\n",
            "[159 | 380.91] loss=2.62 avg=2.82\n",
            "[160 | 383.20] loss=2.71 avg=2.82\n",
            "[161 | 385.49] loss=2.63 avg=2.82\n",
            "[162 | 387.78] loss=2.54 avg=2.82\n",
            "[163 | 390.07] loss=2.80 avg=2.82\n",
            "[164 | 392.35] loss=2.75 avg=2.82\n",
            "[165 | 394.64] loss=2.66 avg=2.81\n",
            "[166 | 396.93] loss=2.64 avg=2.81\n",
            "[167 | 399.23] loss=2.61 avg=2.81\n",
            "[168 | 401.51] loss=2.66 avg=2.81\n",
            "[169 | 403.80] loss=2.47 avg=2.80\n",
            "[170 | 406.08] loss=2.56 avg=2.80\n",
            "[171 | 408.37] loss=2.70 avg=2.80\n",
            "[172 | 410.66] loss=2.58 avg=2.80\n",
            "[173 | 412.95] loss=2.76 avg=2.80\n",
            "[174 | 415.23] loss=2.56 avg=2.79\n",
            "[175 | 417.52] loss=2.54 avg=2.79\n",
            "[176 | 419.81] loss=2.57 avg=2.79\n",
            "[177 | 422.09] loss=2.80 avg=2.79\n",
            "[178 | 424.38] loss=2.57 avg=2.79\n",
            "[179 | 426.67] loss=2.51 avg=2.78\n",
            "[180 | 428.95] loss=2.66 avg=2.78\n",
            "[181 | 431.24] loss=2.89 avg=2.78\n",
            "[182 | 433.52] loss=2.85 avg=2.78\n",
            "[183 | 435.81] loss=2.77 avg=2.78\n",
            "[184 | 438.09] loss=2.55 avg=2.78\n",
            "[185 | 440.37] loss=2.60 avg=2.78\n",
            "[186 | 442.65] loss=2.68 avg=2.78\n",
            "[187 | 444.93] loss=2.80 avg=2.78\n",
            "[188 | 447.22] loss=2.61 avg=2.77\n",
            "[189 | 449.51] loss=2.56 avg=2.77\n",
            "[190 | 451.78] loss=2.49 avg=2.77\n",
            "[191 | 454.06] loss=2.70 avg=2.77\n",
            "[192 | 456.34] loss=2.53 avg=2.77\n",
            "[193 | 458.63] loss=2.49 avg=2.76\n",
            "[194 | 460.91] loss=2.56 avg=2.76\n",
            "[195 | 463.19] loss=2.56 avg=2.76\n",
            "[196 | 465.47] loss=2.57 avg=2.76\n",
            "[197 | 467.75] loss=2.67 avg=2.75\n",
            "[198 | 470.03] loss=2.49 avg=2.75\n",
            "[199 | 472.31] loss=2.45 avg=2.75\n",
            "[200 | 474.60] loss=2.67 avg=2.75\n",
            "======== SAMPLE 1 ========\n",
            " Ill. \n",
            "The Dothraki sent two knights to assist, to the east and west of the camp. The \n",
            "Dothraki had \n",
            "seen a wolf, a direwolf, a wolf of the Dothraki, and a wild boar as well, and were in fear \n",
            "that he too might come on foot. But the ranger had been well armed; armed only with a sword and \n",
            "a spear, he could be an Imp, a direwolf and a wyvern, and with nothing else but two swords and two \n",
            "pliers to his belt he could take the wolf with open swords and close ones, and from what he \n",
            "remembered his horse was a black as blood . . . a bloody black as a plague. All with both arms, \n",
            "Page 405\n",
            "\n",
            "he knew he might have been taken, but not his sword or spear. The Imp was the last thing on Earth that \n",
            "looked like him. Even to the ranger, the Imp looked as though he were standing on fire, and the direwolf \n",
            "was as bad as dead. \n",
            "He thought the Imp looked a little like himself when he came of age. If he died in combat, the \n",
            "Dothraki knew he would die in battle; so they gave him a blood and a whip, and a ring of earth and \n",
            "steel and silver and bone and gold. And the Imp never died. He died for nothing. \n",
            "He looked so alive now, even as he looked the old, as though he could stand on his own ground. \"Where is \n",
            "Robb?\" he asked the ranger. \n",
            "\"A maegi awaits my return,\" the ranger replied with a smile. \n",
            "Catelyn stroked her head. \"I don't understand,\" she muttered. She thought it might have been \n",
            "Jhiqui, but he kept saying, \"Who is this handsome man? What is he doing here? He's \n",
            "doing him harm.\" \n",
            "Jhiqui said nothing, no word escaped her mouth. A maegi had taken out a man's horn, it was said, \n",
            "and they were all saying the same thing: \"He's the Imp.\" She told herself it was all lies, but none of it \n",
            "Page 406\n",
            "\n",
            "was true. They had heard it for years, and not just yet; they were a party of men, they had heard \n",
            "it both ways. The forest was full of it. \n",
            "This one was different; Jhiqui had no horn, no whip, no \n",
            "stone, black as night, in his hand a black dagger. All he had in his hand was an immense wooden hand with a \n",
            "black pointed blade. When the hand slid from the longsword around his waist, his hand flew \n",
            "below in the air with the slightest flick of his whip. The dagger was like a dagger to him; he was \n",
            "only a child, with nothing yet enough steel to use his hands, not enough pain to cut his throat. He was \n",
            "a child with a knife in one hand, as with every Imp in the Seven Kingdoms. That one he had \n",
            "seen. It scared him and frightened him. \n",
            "Catelyn watched him with a frightened fear. It was all she could see, all the way to the \n",
            "top of the huge stone throne looming over the camp. \n",
            "The big stone throne was high and high, the largest there was in all the Seven Kingdoms, and \n",
            "when it swung down \n",
            "the huge stone throne, the world fell. \n",
            "The ranger moved to her feet. \"Robb,\" he called out, \"you are not home now. You have lost your \n",
            "sword, but it is still there, and you know it is there, too. Do you wish to return to \n",
            "Home?\" He stood on the top step and turned and looked at her. \"Where will you go first?\" \n",
            "Catelyn stood on that first step . . . and she had, too, to the west, out in the far distance, looking to the west, \n",
            "and beyond, and all she knew was the world went dark and the Dothraki trembled in the cold of winter. \n",
            "\"Jhiqui?\" \n",
            "\"And the boar,\" the ranger said. \"Jhiqui.\" \n",
            "\"Oh,\" Catelyn said, she was afraid it might be true. \"Jhiqui!\" he said urgently. \"You are too poor \n",
            "to come here.\" \n",
            "Her heart leapt away in her chest. She could not, she could not, not at that, she could not stand \n",
            "it, she had to go. The ranger pointed out that the boar was nowhere in sight, that it had not moved \n",
            "the last ten feet, that the two men were in a\n",
            "\n",
            "[201 | 487.79] loss=2.51 avg=2.74\n",
            "[202 | 490.07] loss=2.48 avg=2.74\n",
            "[203 | 492.34] loss=2.49 avg=2.74\n",
            "[204 | 494.63] loss=2.46 avg=2.73\n",
            "[205 | 496.91] loss=2.88 avg=2.74\n",
            "[206 | 499.19] loss=2.54 avg=2.73\n",
            "[207 | 501.47] loss=2.53 avg=2.73\n",
            "[208 | 503.75] loss=2.49 avg=2.73\n",
            "[209 | 506.03] loss=2.40 avg=2.73\n",
            "[210 | 508.32] loss=2.82 avg=2.73\n",
            "[211 | 510.60] loss=2.63 avg=2.73\n",
            "[212 | 512.88] loss=2.50 avg=2.72\n",
            "[213 | 515.16] loss=2.54 avg=2.72\n",
            "[214 | 517.44] loss=2.70 avg=2.72\n",
            "[215 | 519.72] loss=2.22 avg=2.71\n",
            "[216 | 522.01] loss=2.44 avg=2.71\n",
            "[217 | 524.29] loss=2.60 avg=2.71\n",
            "[218 | 526.57] loss=2.61 avg=2.71\n",
            "[219 | 528.85] loss=2.56 avg=2.71\n",
            "[220 | 531.14] loss=2.61 avg=2.71\n",
            "[221 | 533.43] loss=2.66 avg=2.71\n",
            "[222 | 535.71] loss=2.50 avg=2.70\n",
            "[223 | 537.99] loss=2.52 avg=2.70\n",
            "[224 | 540.27] loss=2.53 avg=2.70\n",
            "[225 | 542.56] loss=2.56 avg=2.70\n",
            "[226 | 544.84] loss=2.43 avg=2.70\n",
            "[227 | 547.13] loss=2.73 avg=2.70\n",
            "[228 | 549.41] loss=2.36 avg=2.69\n",
            "[229 | 551.70] loss=2.57 avg=2.69\n",
            "[230 | 553.99] loss=2.39 avg=2.69\n",
            "[231 | 556.28] loss=2.43 avg=2.68\n",
            "[232 | 558.57] loss=2.40 avg=2.68\n",
            "[233 | 560.86] loss=2.67 avg=2.68\n",
            "[234 | 563.14] loss=2.54 avg=2.68\n",
            "[235 | 565.43] loss=2.35 avg=2.68\n",
            "[236 | 567.71] loss=2.38 avg=2.67\n",
            "[237 | 570.01] loss=2.48 avg=2.67\n",
            "[238 | 572.29] loss=2.44 avg=2.67\n",
            "[239 | 574.57] loss=2.64 avg=2.67\n",
            "[240 | 576.87] loss=2.45 avg=2.67\n",
            "[241 | 579.16] loss=2.54 avg=2.66\n",
            "[242 | 581.45] loss=2.60 avg=2.66\n",
            "[243 | 583.74] loss=2.43 avg=2.66\n",
            "[244 | 586.02] loss=2.46 avg=2.66\n",
            "[245 | 588.31] loss=2.45 avg=2.66\n",
            "[246 | 590.59] loss=2.70 avg=2.66\n",
            "[247 | 592.88] loss=2.39 avg=2.65\n",
            "[248 | 595.18] loss=2.47 avg=2.65\n",
            "[249 | 597.46] loss=2.56 avg=2.65\n",
            "[250 | 599.75] loss=2.56 avg=2.65\n",
            "[251 | 602.04] loss=2.71 avg=2.65\n",
            "[252 | 604.33] loss=2.48 avg=2.65\n",
            "[253 | 606.62] loss=2.44 avg=2.65\n",
            "[254 | 608.91] loss=2.49 avg=2.64\n",
            "[255 | 611.21] loss=2.29 avg=2.64\n",
            "[256 | 613.50] loss=2.35 avg=2.64\n",
            "[257 | 615.79] loss=2.35 avg=2.63\n",
            "[258 | 618.10] loss=2.36 avg=2.63\n",
            "[259 | 620.38] loss=2.44 avg=2.63\n",
            "[260 | 622.67] loss=2.39 avg=2.63\n",
            "[261 | 624.96] loss=2.10 avg=2.62\n",
            "[262 | 627.24] loss=2.26 avg=2.62\n",
            "[263 | 629.54] loss=2.57 avg=2.62\n",
            "[264 | 631.83] loss=2.43 avg=2.61\n",
            "[265 | 634.12] loss=2.62 avg=2.61\n",
            "[266 | 636.40] loss=2.14 avg=2.61\n",
            "[267 | 638.70] loss=2.58 avg=2.61\n",
            "[268 | 640.98] loss=2.31 avg=2.61\n",
            "[269 | 643.27] loss=2.43 avg=2.60\n",
            "[270 | 645.56] loss=2.33 avg=2.60\n",
            "[271 | 647.85] loss=2.29 avg=2.60\n",
            "[272 | 650.13] loss=2.12 avg=2.59\n",
            "[273 | 652.43] loss=2.27 avg=2.59\n",
            "[274 | 654.72] loss=2.44 avg=2.59\n",
            "[275 | 657.01] loss=2.19 avg=2.58\n",
            "[276 | 659.30] loss=2.30 avg=2.58\n",
            "[277 | 661.59] loss=2.33 avg=2.58\n",
            "[278 | 663.89] loss=2.13 avg=2.57\n",
            "[279 | 666.18] loss=2.43 avg=2.57\n",
            "[280 | 668.48] loss=2.45 avg=2.57\n",
            "[281 | 670.77] loss=2.40 avg=2.57\n",
            "[282 | 673.07] loss=2.53 avg=2.57\n",
            "[283 | 675.36] loss=2.42 avg=2.57\n",
            "[284 | 677.65] loss=2.36 avg=2.56\n",
            "[285 | 679.96] loss=2.40 avg=2.56\n",
            "[286 | 682.24] loss=2.37 avg=2.56\n",
            "[287 | 684.53] loss=2.25 avg=2.56\n",
            "[288 | 686.83] loss=2.16 avg=2.55\n",
            "[289 | 689.12] loss=2.39 avg=2.55\n",
            "[290 | 691.42] loss=2.36 avg=2.55\n",
            "[291 | 693.71] loss=2.38 avg=2.55\n",
            "[292 | 696.00] loss=2.24 avg=2.54\n",
            "[293 | 698.29] loss=2.45 avg=2.54\n",
            "[294 | 700.58] loss=2.51 avg=2.54\n",
            "[295 | 702.87] loss=2.65 avg=2.54\n",
            "[296 | 705.17] loss=2.39 avg=2.54\n",
            "[297 | 707.45] loss=2.25 avg=2.54\n",
            "[298 | 709.74] loss=2.39 avg=2.54\n",
            "[299 | 712.03] loss=2.52 avg=2.54\n",
            "[300 | 714.33] loss=2.15 avg=2.53\n",
            "======== SAMPLE 1 ========\n",
            " \"she's never been afraid to wear gloves,\" she said. \"They're beautiful, I can see them in the sun and the wind, I can't wait to get them on when I grow old.\" \n",
            "The little thing looked as if she'd shriveled up a little. \"I know what you are talking about,\" she reminded her \n",
            "daughter, her voice sharp and bitter. She took a bite of meat from one of the tables behind her, and bit it. \n",
            "\"My father's butcher says that if you're ever in need of a knife, this is the place.\" \n",
            "She was pleasantly surprised when she saw that she had grown to trust her daughter in turn. \"Is that my fault?\" \n",
            "\"Yes,\" she said doubtfully. \"How long before I cut your legs?\" \n",
            "\"When you'll be old enough to feed?\" \n",
            "\"Once you're old enough to ride,\" Ser Rodrik said with a shrug. \n",
            "Page 345\n",
            "\n",
            "\"The butcher said my leg wouldn't grow back, so I took a half-dozen sword blades from the table,\" \n",
            "she said. \"I chopped off the middle finger, and there you are: a small knife.\" \n",
            "\"The blade is not meant to be stabbed,\" Ser Rodrik said with a sharp smile he gave his apprentice. \"I \n",
            "liked the look, but the look alone is not enough.\" \n",
            "\"Do I want to be your apprentice?\" Maester Pycelle asked. His look was as nervous and unadmirable as the way he seemed \n",
            "to him. \n",
            "Ser Rodrik looked back. \"I want to be your steward after you . . . and your friend, too.\" \n",
            "\"Will I help you become your steward?\" \n",
            "\"Yes.\" \n",
            "\"You'll have to tell me when I get back.\" \n",
            "\"I'll pick you up at your place.\" \n",
            "\"No, maester,\" the steward said, smiling. \n",
            "\"I'll be more curious to see if you have any knives when I go to see you in person,\" she said. \"We \n",
            "must be careful.\" \n",
            "The little old man gave a nervous shake of his head. \"You're not too young for this, child. I would . \n",
            "think you were fifteen when my sword of choice came out. I can remember.\" He took a deep breath, \n",
            "and the knife went out of his hand. \"The blade of the sword I want you to have is called the \n",
            "Ser Vardis.\" \n",
            "\"A knife?\" \n",
            "\"It is not a sword. Your choice. I will ask you to return it at once and see if it is in your \n",
            "ask.\" \n",
            "\"Yes,\" the little man said, grinning. \"It is a dagger. I would have never found this in my father's forge, even \n",
            "before he built the Seven Kingdoms.\" \n",
            "Ser Rodrik smiled. \"Perhaps your father was a wizard, and a few of your friends were, and \n",
            "you will find the truth of it now, child.\" \n",
            "Afterward her granddaughter followed them all with curious sullen looks. \"I hope you're not scared away by their swords, \n",
            "Mother. It's not like you to leave a sword on the table after you reach seventy!\" \n",
            "Page 346\n",
            "\n",
            "\"Yes. Very good, I'm afraid.\" The little boy picked up the dagger and looked at it with \n",
            "deep suspicions. \"How many years did I forget?\" he asked. \n",
            "\"\"You will know when I'm done with them.\" \n",
            "\"How many years?\" \n",
            "\"I wish.\" \n",
            "The little lad gave her a sharp curt shrug, his mouth a little dry. \"They were my blades first, my lady. And I did not \n",
            "make use of them.\" He handed him the dagger. \n",
            "\"What has been stolen, and what will it be?\" \n",
            "\"A hundred thousand gold pieces, or twenty for each sword. The gold is enough for a thousand.\" \n",
            "\"And you are the king.\" \n",
            "\"A hundred thousand,\" the little boy said. \"This is a gift from the gods, Mother.\" \n",
            "Ser Rodrik gave a long sigh. \"It's not like King Tully to give gifts to the king every time he visits your \n",
            "place, I am told. Even the gods are always curious.\" \n",
            "\"You should have seen them on the city walls,\" Maester Colemon reminded her sadly. \"My father would never have \n",
            "seen it.\" She was reminded of one of King Robert's crimes, for that was when she had first learned \n",
            "of the king. It was one of her father's crimes that she had not been able to speak to him, no one \n",
            "should have a power to take away a gift.\" \n",
            "Page 347\n",
            "\n",
            "\n",
            "\n",
            "[301 | 727.82] loss=2.10 avg=2.53\n",
            "[302 | 730.11] loss=2.31 avg=2.53\n",
            "[303 | 732.39] loss=2.39 avg=2.53\n",
            "[304 | 734.68] loss=2.48 avg=2.52\n",
            "[305 | 736.97] loss=2.30 avg=2.52\n",
            "[306 | 739.25] loss=2.26 avg=2.52\n",
            "[307 | 741.55] loss=2.12 avg=2.52\n",
            "[308 | 743.83] loss=2.18 avg=2.51\n",
            "[309 | 746.12] loss=2.42 avg=2.51\n",
            "[310 | 748.40] loss=2.19 avg=2.51\n",
            "[311 | 750.69] loss=2.24 avg=2.50\n",
            "[312 | 752.97] loss=2.28 avg=2.50\n",
            "[313 | 755.26] loss=2.36 avg=2.50\n",
            "[314 | 757.54] loss=2.27 avg=2.50\n",
            "[315 | 759.82] loss=2.20 avg=2.50\n",
            "[316 | 762.09] loss=1.93 avg=2.49\n",
            "[317 | 764.38] loss=2.50 avg=2.49\n",
            "[318 | 766.66] loss=2.07 avg=2.49\n",
            "[319 | 768.95] loss=2.49 avg=2.49\n",
            "[320 | 771.23] loss=2.45 avg=2.48\n",
            "[321 | 773.52] loss=2.28 avg=2.48\n",
            "[322 | 775.80] loss=2.10 avg=2.48\n",
            "[323 | 778.09] loss=2.16 avg=2.48\n",
            "[324 | 780.38] loss=2.16 avg=2.47\n",
            "[325 | 782.67] loss=2.22 avg=2.47\n",
            "[326 | 784.95] loss=2.30 avg=2.47\n",
            "[327 | 787.23] loss=2.18 avg=2.46\n",
            "[328 | 789.51] loss=2.34 avg=2.46\n",
            "[329 | 791.79] loss=2.33 avg=2.46\n",
            "[330 | 794.08] loss=2.28 avg=2.46\n",
            "[331 | 796.36] loss=2.15 avg=2.46\n",
            "[332 | 798.65] loss=2.67 avg=2.46\n",
            "[333 | 800.94] loss=2.51 avg=2.46\n",
            "[334 | 803.23] loss=2.17 avg=2.46\n",
            "[335 | 805.52] loss=2.33 avg=2.46\n",
            "[336 | 807.81] loss=2.15 avg=2.45\n",
            "[337 | 810.09] loss=2.50 avg=2.45\n",
            "[338 | 812.38] loss=2.16 avg=2.45\n",
            "[339 | 814.67] loss=2.09 avg=2.45\n",
            "[340 | 816.96] loss=2.24 avg=2.44\n",
            "[341 | 819.25] loss=2.31 avg=2.44\n",
            "[342 | 821.54] loss=2.27 avg=2.44\n",
            "[343 | 823.83] loss=2.28 avg=2.44\n",
            "[344 | 826.12] loss=2.24 avg=2.44\n",
            "[345 | 828.42] loss=2.50 avg=2.44\n",
            "[346 | 830.70] loss=2.56 avg=2.44\n",
            "[347 | 832.99] loss=2.37 avg=2.44\n",
            "[348 | 835.27] loss=2.19 avg=2.44\n",
            "[349 | 837.56] loss=2.18 avg=2.43\n",
            "[350 | 839.86] loss=2.39 avg=2.43\n",
            "[351 | 842.15] loss=2.29 avg=2.43\n",
            "[352 | 844.44] loss=2.16 avg=2.43\n",
            "[353 | 846.73] loss=2.38 avg=2.43\n",
            "[354 | 849.02] loss=2.12 avg=2.42\n",
            "[355 | 851.32] loss=2.21 avg=2.42\n",
            "[356 | 853.61] loss=2.03 avg=2.42\n",
            "[357 | 855.90] loss=2.10 avg=2.42\n",
            "[358 | 858.19] loss=2.33 avg=2.41\n",
            "[359 | 860.48] loss=2.05 avg=2.41\n",
            "[360 | 862.77] loss=2.19 avg=2.41\n",
            "[361 | 865.06] loss=2.12 avg=2.41\n",
            "[362 | 867.35] loss=2.51 avg=2.41\n",
            "[363 | 869.64] loss=2.31 avg=2.41\n",
            "[364 | 871.93] loss=2.16 avg=2.40\n",
            "[365 | 874.22] loss=2.20 avg=2.40\n",
            "[366 | 876.51] loss=2.07 avg=2.40\n",
            "[367 | 878.80] loss=2.16 avg=2.39\n",
            "[368 | 881.10] loss=1.97 avg=2.39\n",
            "[369 | 883.38] loss=2.21 avg=2.39\n",
            "[370 | 885.67] loss=2.09 avg=2.39\n",
            "[371 | 887.97] loss=2.09 avg=2.38\n",
            "[372 | 890.26] loss=2.04 avg=2.38\n",
            "[373 | 892.54] loss=2.11 avg=2.38\n",
            "[374 | 894.84] loss=2.05 avg=2.37\n",
            "[375 | 897.12] loss=2.02 avg=2.37\n",
            "[376 | 899.41] loss=2.15 avg=2.37\n",
            "[377 | 901.70] loss=1.98 avg=2.36\n",
            "[378 | 904.00] loss=2.21 avg=2.36\n",
            "[379 | 906.29] loss=2.06 avg=2.36\n",
            "[380 | 908.58] loss=2.21 avg=2.36\n",
            "[381 | 910.86] loss=2.22 avg=2.36\n",
            "[382 | 913.15] loss=1.92 avg=2.35\n",
            "[383 | 915.44] loss=2.02 avg=2.35\n",
            "[384 | 917.73] loss=2.09 avg=2.34\n",
            "[385 | 920.01] loss=2.23 avg=2.34\n",
            "[386 | 922.28] loss=2.16 avg=2.34\n",
            "[387 | 924.57] loss=2.05 avg=2.34\n",
            "[388 | 926.86] loss=1.97 avg=2.34\n",
            "[389 | 929.14] loss=2.05 avg=2.33\n",
            "[390 | 931.41] loss=2.00 avg=2.33\n",
            "[391 | 933.70] loss=2.00 avg=2.33\n",
            "[392 | 935.98] loss=1.81 avg=2.32\n",
            "[393 | 938.27] loss=2.34 avg=2.32\n",
            "[394 | 940.55] loss=1.85 avg=2.32\n",
            "[395 | 942.84] loss=1.76 avg=2.31\n",
            "[396 | 945.12] loss=2.11 avg=2.31\n",
            "[397 | 947.41] loss=2.09 avg=2.31\n",
            "[398 | 949.70] loss=2.49 avg=2.31\n",
            "[399 | 951.99] loss=1.60 avg=2.30\n",
            "[400 | 954.28] loss=2.14 avg=2.30\n",
            "======== SAMPLE 1 ========\n",
            "Her lady will know I be dead \n",
            "\"No,\" she managed, \"the dead bear witness.\" \n",
            "Gaunt and weary, Sansa stood over the bodies. \"She must have heard, I'd heard it all first.\" \n",
            "\"She must have,\" Bran said. \n",
            "Bran reached forward silently, to make certain she did not fall to her death. He found the wood near the bridge \n",
            "and lifted the torch high over the hearth, the shadows of trees in the wind. The flickering was like a \n",
            "dream, and Bran could not help but smile when he glimpsed the dark, but there he was, half a \n",
            "field away, his cloak glowing in the sunlight. His handmaids carried him off in his own, leaving him no choice \n",
            "to run. He watched them carry him back to the stables, the grey light of sunset in the grey \n",
            "sky, long after the men were gone. He watched Bran in the sunlight, and for an instant he \n",
            "realized what a strange relief it must have felt to see him with their dead. His sister and his best friend \n",
            "Tyrion Lannister were dead behind them and the old gods and men of Winterfell, and beyond \n",
            "them. \n",
            "Maester Luwin had already gone to him, and Bran knew they were safe here in their hearts. But it \n",
            "was no comfort that was easy out here in the open, he knew, even down to the dungeons. It felt as if he \n",
            "had been strangled. \"We'll be taking them back someday,\" Sansa told him. \n",
            "Page 459\n",
            "\n",
            "\"No,\" Bran said, \"the dead will pay. If we return to the castle in good time, I'd sooner kill them than risk \n",
            "my life. But I'll be here with all the \n",
            "Page 460\n",
            "\n",
            "wizards you've seen in your life.\" He hesitated a long moment, then decided it was better not to talk of it \n",
            "anymore. \"We'll take these two men. The girls and the boy with the girl with them. The rest is \n",
            "off.\" Sansa was right. The Lannisters would be the last damn thing the castle had to offer. \n",
            "Bran thought of all those years ago when he heard the news of Lord Eddard's death. It had shocked \n",
            "him. It made him grieve. \"Oh, Father,\" he said, shaking. \"There was a certain boy I had to protect.\" \n",
            "\"I had to? Is it a boy?\" Maester Luwin asked. \"Oh, yes it is.\" \n",
            "\"I had to protect him. That's what we did. We carried the boy here from the castle, you did not \n",
            "disclose us. He never came back. So you do. The others have all come forward. Those who did not come \n",
            "forward. Those who did.\" \n",
            "It gave Bran an ancient look. \"Father,\" he said, wiping his red eyes, \"I have pity on those who did come \n",
            "forward.\" \n",
            "\"Pray allow me to be merciful,\" Maester Luwin said. \"Those men were the true warriors who took hold of \n",
            "the boy on the morrow, and who were not yet dead. A thousand deaths would have been you and me, as I \n",
            "am certain . . . and the Kingslayer. He would have taken them both dead before he could move freely in the south.\" \n",
            "\"And the king?\" \n",
            "\"He came into my solar as a king,\" Maester Luwin said. \"His bloodriders will be here soon enough.\" \n",
            "He had a look on his face. The faces of the children of the forest. \"They won't,\" Bran said. \"I can't \n",
            "watch someone kill a baby.\" \n",
            "\"The Others,\" Maester Luwin said. \"The Others have a better grasp of our thinking than the \n",
            "Others have of yours.\" \n",
            "\"There are some who call us better,\" Bran said. \"Not all of us. Some take it for granted, some for a certainty. If we \n",
            "could all agree that the Others are no more than a figment of your imagination, then perhaps we might still be \n",
            "men who can do what the Others do best.\" He glanced at the children, then back at Bran. \"They do seem \n",
            "younger and darker, perhaps even more willful. Some of them seem to have forgotten all about the \n",
            "bravado.\" \n",
            "Page 461\n",
            "\n",
            "Maester Luwin lifted his eyes. \"Your sisters do remember well. What were they thinking again of the \n",
            "jungle?\" \n",
            "\"The Others.\" \n",
            "Maester Luwin frowned. \"There are truth be told about their children. The Others had fostered children on \n",
            "mynee days. There were reports of a direwolf\n",
            "\n",
            "[401 | 967.56] loss=1.91 avg=2.29\n",
            "[402 | 969.86] loss=2.16 avg=2.29\n",
            "[403 | 972.16] loss=2.12 avg=2.29\n",
            "[404 | 974.46] loss=2.13 avg=2.29\n",
            "[405 | 976.75] loss=2.22 avg=2.29\n",
            "[406 | 979.04] loss=1.87 avg=2.29\n",
            "[407 | 981.33] loss=2.17 avg=2.28\n",
            "[408 | 983.64] loss=1.85 avg=2.28\n",
            "[409 | 985.94] loss=1.90 avg=2.28\n",
            "[410 | 988.23] loss=2.03 avg=2.27\n",
            "[411 | 990.52] loss=1.62 avg=2.27\n",
            "[412 | 992.80] loss=1.77 avg=2.26\n",
            "[413 | 995.09] loss=1.91 avg=2.26\n",
            "[414 | 997.38] loss=1.67 avg=2.25\n",
            "[415 | 999.66] loss=1.93 avg=2.25\n",
            "[416 | 1001.95] loss=2.14 avg=2.25\n",
            "[417 | 1004.23] loss=2.10 avg=2.25\n",
            "[418 | 1006.52] loss=1.87 avg=2.24\n",
            "[419 | 1008.80] loss=2.04 avg=2.24\n",
            "[420 | 1011.08] loss=1.78 avg=2.24\n",
            "[421 | 1013.37] loss=1.89 avg=2.23\n",
            "[422 | 1015.66] loss=1.88 avg=2.23\n",
            "[423 | 1017.94] loss=1.92 avg=2.23\n",
            "[424 | 1020.23] loss=2.17 avg=2.22\n",
            "[425 | 1022.51] loss=2.21 avg=2.22\n",
            "[426 | 1024.80] loss=2.01 avg=2.22\n",
            "[427 | 1027.09] loss=2.00 avg=2.22\n",
            "[428 | 1029.37] loss=2.13 avg=2.22\n",
            "[429 | 1031.67] loss=2.23 avg=2.22\n",
            "[430 | 1033.96] loss=1.86 avg=2.22\n",
            "[431 | 1036.26] loss=1.79 avg=2.21\n",
            "[432 | 1038.56] loss=1.95 avg=2.21\n",
            "[433 | 1040.85] loss=2.08 avg=2.21\n",
            "[434 | 1043.15] loss=1.77 avg=2.20\n",
            "[435 | 1045.45] loss=1.95 avg=2.20\n",
            "[436 | 1047.76] loss=1.97 avg=2.20\n",
            "[437 | 1050.06] loss=2.07 avg=2.20\n",
            "[438 | 1052.35] loss=1.66 avg=2.19\n",
            "[439 | 1054.64] loss=2.21 avg=2.19\n",
            "[440 | 1056.93] loss=1.90 avg=2.19\n",
            "[441 | 1059.23] loss=2.12 avg=2.19\n",
            "[442 | 1061.53] loss=1.89 avg=2.18\n",
            "[443 | 1063.82] loss=2.05 avg=2.18\n",
            "[444 | 1066.13] loss=1.87 avg=2.18\n",
            "[445 | 1068.42] loss=1.93 avg=2.18\n",
            "[446 | 1070.71] loss=1.76 avg=2.17\n",
            "[447 | 1073.00] loss=1.81 avg=2.17\n",
            "[448 | 1075.30] loss=2.19 avg=2.17\n",
            "[449 | 1077.59] loss=1.84 avg=2.17\n",
            "[450 | 1079.88] loss=2.20 avg=2.17\n",
            "[451 | 1082.17] loss=2.07 avg=2.17\n",
            "[452 | 1084.47] loss=1.72 avg=2.16\n",
            "[453 | 1086.76] loss=1.90 avg=2.16\n",
            "[454 | 1089.05] loss=1.72 avg=2.15\n",
            "[455 | 1091.35] loss=1.82 avg=2.15\n",
            "[456 | 1093.64] loss=1.83 avg=2.15\n",
            "[457 | 1095.93] loss=2.17 avg=2.15\n",
            "[458 | 1098.22] loss=1.84 avg=2.15\n",
            "[459 | 1100.52] loss=1.87 avg=2.14\n",
            "[460 | 1102.80] loss=1.88 avg=2.14\n",
            "[461 | 1105.09] loss=1.94 avg=2.14\n",
            "[462 | 1107.39] loss=2.23 avg=2.14\n",
            "[463 | 1109.69] loss=1.84 avg=2.14\n",
            "[464 | 1111.98] loss=1.73 avg=2.13\n",
            "[465 | 1114.28] loss=1.84 avg=2.13\n",
            "[466 | 1116.57] loss=1.92 avg=2.13\n",
            "[467 | 1118.86] loss=1.80 avg=2.12\n",
            "[468 | 1121.15] loss=1.69 avg=2.12\n",
            "[469 | 1123.43] loss=2.06 avg=2.12\n",
            "[470 | 1125.72] loss=1.41 avg=2.11\n",
            "[471 | 1128.01] loss=1.72 avg=2.11\n",
            "[472 | 1130.30] loss=1.60 avg=2.10\n",
            "[473 | 1132.60] loss=1.71 avg=2.10\n",
            "[474 | 1134.89] loss=2.19 avg=2.10\n",
            "[475 | 1137.18] loss=1.63 avg=2.09\n",
            "[476 | 1139.46] loss=1.58 avg=2.09\n",
            "[477 | 1141.75] loss=1.72 avg=2.09\n",
            "[478 | 1144.04] loss=1.80 avg=2.08\n",
            "[479 | 1146.34] loss=1.94 avg=2.08\n",
            "[480 | 1148.63] loss=2.21 avg=2.08\n",
            "[481 | 1150.92] loss=2.17 avg=2.08\n",
            "[482 | 1153.20] loss=1.69 avg=2.08\n",
            "[483 | 1155.50] loss=1.78 avg=2.08\n",
            "[484 | 1157.79] loss=1.95 avg=2.07\n",
            "[485 | 1160.08] loss=1.90 avg=2.07\n",
            "[486 | 1162.37] loss=1.76 avg=2.07\n",
            "[487 | 1164.65] loss=1.82 avg=2.07\n",
            "[488 | 1166.94] loss=2.06 avg=2.07\n",
            "[489 | 1169.24] loss=1.82 avg=2.06\n",
            "[490 | 1171.52] loss=0.97 avg=2.05\n",
            "[491 | 1173.81] loss=1.72 avg=2.05\n",
            "[492 | 1176.10] loss=1.57 avg=2.05\n",
            "[493 | 1178.39] loss=1.90 avg=2.04\n",
            "[494 | 1180.68] loss=1.67 avg=2.04\n",
            "[495 | 1182.96] loss=2.15 avg=2.04\n",
            "[496 | 1185.25] loss=1.67 avg=2.04\n",
            "[497 | 1187.53] loss=1.70 avg=2.03\n",
            "[498 | 1189.82] loss=1.83 avg=2.03\n",
            "[499 | 1192.11] loss=1.73 avg=2.03\n",
            "[500 | 1194.40] loss=1.87 avg=2.03\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50e701f-e1dc-4fb7-80b1-8d3d5f15dc6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? \n",
            "Dany wanted to think beyond the wall. She had no illusions about the way the gods knew of her \n",
            "death. She had come before the gods, she was held captive, and had lost her sight . . . and her \n",
            "mind . . . And she was free. \n",
            "There was no place to turn, no place to hide, no place to beg, no place to forgive. She had no rights, no \n",
            "rights at all. \n",
            "Spirit children \n",
            "Page 557\n",
            "\n",
            "had been raised among the dead gods, but they had not dared to enter the solar system. They \n",
            "had been abandoned, abandoned for a few days, then brought forth to life . . . and now she\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}